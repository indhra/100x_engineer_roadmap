# Week 1: LLM Fundamentals

> **Goal:** Understand how LLMs work at an architect level - not to build one, but to make informed decisions about using them.

---

## ğŸ¯ Learning Objectives

By end of Week 1, you will:
- [ ] Explain transformer architecture without looking at notes
- [ ] Understand tokenization trade-offs (BPE vs WordPiece vs SentencePiece)
- [ ] Reason about embeddings as "meaning coordinates"
- [ ] Make context window decisions for production systems
- [ ] Estimate costs and latency for LLM deployments
- [ ] Choose between open vs closed models for different use cases

---

## ğŸ“ Week Structure

```
week1_llm_fundamentals/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ concepts/
â”‚   â”œâ”€â”€ 01_transformer_architecture.md
â”‚   â”œâ”€â”€ 02_tokenization.md
â”‚   â”œâ”€â”€ 03_embeddings.md
â”‚   â”œâ”€â”€ 04_context_windows.md
â”‚   â”œâ”€â”€ 05_inference_vs_training.md
â”‚   â””â”€â”€ 06_open_vs_closed.md
â”œâ”€â”€ exercises/
â”‚   â”œâ”€â”€ tokenization_lab.ipynb
â”‚   â”œâ”€â”€ embeddings_lab.ipynb
â”‚   â””â”€â”€ cost_estimation.py
â””â”€â”€ architect_decisions.md       # Your design decisions log
```

---

## ğŸ“… Daily Plan

| Day | Focus | Activity |
|-----|-------|----------|
| Day 1 | Transformer Architecture | Concept deep-dive, mental models |
| Day 2 | Tokenization | BPE/WordPiece/SentencePiece + hands-on |
| Day 3 | Embeddings | Vector space intuition + similarity lab |
| Day 4 | Context Windows | Strategies, trade-offs, production patterns |
| Day 5 | Inference vs Training | Cost/latency analysis |
| Day 6 | Open vs Closed Models | Decision framework |
| Day 7 | Review & Integration | Architect questions, synthesis |

---

## ğŸ§  Mental Models to Build

1. **LLM as "Compressed Internet + Reasoning Engine"**
   - Not a knowledge base, but a pattern recognizer
   - Trained on internet-scale text, learned to predict next token
   - "Reasoning" emerges from scale + training

2. **Tokens as "Atoms of Language"**
   - The fundamental unit LLMs see
   - Not words, not characters - subwords
   - Different tokenizers = different "atomic structure"

3. **Embeddings as "Meaning Coordinates"**
   - Each token/phrase = point in high-dimensional space
   - Similar meanings = nearby in space
   - Foundation of semantic search

---

## âœ… Completion Checklist

- [ ] All 6 concept documents read and understood
- [ ] Hands-on exercises completed
- [ ] Architect questions answered in `architect_decisions.md`
- [ ] Can explain concepts to someone else

---

*Started: January 31, 2026*
